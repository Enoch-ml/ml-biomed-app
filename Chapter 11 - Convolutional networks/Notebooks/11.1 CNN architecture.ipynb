{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks of CNN architectures\n",
    "\n",
    "The basic components of a convolutional neural networks are convolutional layers and downsampling operations. In this notebook we will downsample through max pooling.\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "The convolutional operation works similarly to what you have previously seen for hand engineered localised feature descriptors (week 8) such as the Gray level co-occurrence matrix and Sobel, Laplacian and Gaussian filters.\n",
    "\n",
    "These work by translating a hand engineered filter kernel across an image, at each position, multiplying each element of the filter with the element of the image that they overlap with. This operation is shown below for the vertical sobel edge filter, for the first filter location:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gb2QaE8lW6GgNFwCEk5mJqP0Uo91xxn8\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The second (translated one step to the right)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gcsv7870kjTCwnUV043k2u4iM2uydzBn\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "This continues until the filter has been fit at all possible locations in the image. Until the final output of the convolution is another image:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1geRXevPNs_sSMeKWyEnEEKFszz3ptqcV\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "This will be slightly smaller than the input image by default, since it corresponds to all locations in the input image on which the filter can be centered. This excludes the outer rows and columns of the image.\n",
    "\n",
    "By default however, Pytorch will return you an image of the same size by using padding:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ggS0vucaH_mM9X1aBTRX4lSEDnZlmyZg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**The key difference between CNNs and traditional filters is that CNNs learn the best filters for a specific feature recognition problem, whereas traditional feature detectors are hand engineered**\n",
    "\n",
    "### Max pooling\n",
    "\n",
    "The next most important component of a CNN is downsampling. Downsampling allows a CNN to increase its _receptive field_ :\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gV1ZqsV5JGkWys5H0bDcnw9pgR-EYtvf\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gYKvI9ULXFhdyHSGp4XoUmyMFmCijbsI\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "As you saw in the previous section the filters themselves are small. The only way the network can 'see' the full context of the image is by aggregating aceoss layers, and by downsampling at regular intervals. In this way, as you go through the network the filters will learn more and more complex textures, at larger scales, until they can recognise whole objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding CNNs in Pytorch\n",
    "\n",
    "You already saw last week how to set up a basic fullly connected network, choose a cost function and write a training loop. All this stays the same for CNNs. All you need in addition is convolutional and max pool layers.\n",
    "\n",
    "Where in PyTorch, A 2D convolution class is defined within the `torch.nn` module as follows:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12hvQSk-kCsPWTnEE16KKPkA0zo1R3Wzc\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "And the maxpool function in pytorch is [```nn.MaxPool2d```](https://pytorch.org/docs/stable/nn.html?highlight=maxpool#torch.nn.MaxPool2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs of 2D convolutional layers must\n",
    "have a shape $N\\times C\\times H\\times W$, where $N$ is the number of images\n",
    "in a batch, $C$ is the number of channels, $H$ is the image height and\n",
    "$W$ is the image width. The code below creates a random image and passes it through a convolutional layer. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create a random input image\n",
    "input_image = torch.randint(0, 255, (1, 1, 64, 64)).float()\n",
    "print('Input size: ',input_image.shape)\n",
    "\n",
    "# create a convolutional layer\n",
    "conv = torch.nn.Conv2d(1,8,5,padding=2)\n",
    "\n",
    "# pass the random image through the convolutional layer\n",
    "output = conv(input_image)\n",
    "print('Output size: ',output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 1:__ Answer the following questions:\n",
    "* What is the batch size and number of channels of input image\n",
    "* How many channels does the convolutional layer output?\n",
    "* What happens if you change the `padding` parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 2:__ Implement a convolutional layer as follows:\n",
    "* Create a random image with spatial dimensions $100\\times 100$ and 3 channels;\n",
    "* Implement a convolutional layer that outputs 5 channels and has a\n",
    "kernel size of $3\\times 3$. Pass the image through it. Print out the dimensions\n",
    "of the results\n",
    "* Change the convolutional layer so that its output has spatial dimensions $100\\times 100$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random input image\n",
    "input_image2 = None\n",
    "print('Input size: ',input_image2.shape)\n",
    "\n",
    "# create a convolutional layer\n",
    "conv2 = None\n",
    "\n",
    "# pass the random image through the convolutional layer\n",
    "output2 = None\n",
    "print('Output size: ',output2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 3:__ Change the stride of the convolutional layer, so that the output has spatial dimensions $20\\times 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a convolutional layer\n",
    "conv3 = None\n",
    "\n",
    "# pass the random image through the convolutional layer\n",
    "output3 = None\n",
    "print('Output size: ',output3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 4:__ Instead of changing the stride, implement a max-pooling operation to reduce the dimension of the output of the convolutional layer with stride 1 to $20\\times 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a convolutional layer\n",
    "conv4 = None\n",
    "\n",
    "# pass the random image through the convolutional layer\n",
    "output4 = None\n",
    "print('Output size: ',output4.shape)\n",
    "\n",
    "# max pooling\n",
    "maxpool = None\n",
    "downsampled = None\n",
    "print(downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "You are given code that implements this CNN architecture: \n",
    "<img src=\"images/CNN.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "The `CNNModel` has four blocks, two convolutional blocks followed by two linear blocks. Each block is implemented using `nn.Sequential`. The convolutional blocks consist of convolutional layer, ReLU activation and Pooling layer. The linear blocks consist of linear layer and a ReLU activation.\n",
    "\n",
    "Run the code and study the size of the input and the outputs of each block. Note that the shape of the output of each block needs to match input of the following block. After second convolutional block, we also need to reshape the output into a vector using `view` to be able to feed it to a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# CNN architecture\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1,8,5,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(8,16,5,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        self.fc_block1 = nn.Sequential(\n",
    "            nn.Linear(16*16*16, 128),\n",
    "            nn.ReLU())\n",
    "        self.fc_block2 = nn.Sequential(\n",
    "            nn.Linear(128,10),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        print('Output 1: ', x.shape)\n",
    "        x = self.conv_block2(x)\n",
    "        print('Output 2: ', x.shape)\n",
    "        x = x.view(-1, 16*16*16)\n",
    "        x = self.fc_block1(x)\n",
    "        print('Output 3: ', x.shape)\n",
    "        x = self.fc_block2(x)\n",
    "        print('Output 4: ', x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# input image\n",
    "input_image = torch.randint(0, 255, (1, 1, 64, 64)).float()\n",
    "print('Input: ', input_image.shape)\n",
    "# create CNN model\n",
    "net = CNNModel()\n",
    "# predict output for input_image\n",
    "o = net(input_image)\n",
    "# shape is as expected\n",
    "print('Final output: ', o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `CNNmodel` by changing each of the following:\n",
    "* the name of the model to `CNNmodel2`;\n",
    "* the number of output channels of the first and second convolutional\n",
    "block to 4 and 6 respectively;\n",
    "* the number of outputs of the first and second fully connected block\n",
    "to 32 and 2, respectively\n",
    "\n",
    "Create an instance `net2` of the new model. Perform forward pass with `input_image`. Check that you have 2 outputs at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# CNN architecture\n",
    "class CNNModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel2, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1,None,5,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(None,None,5,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        self.fc_block1 = nn.Sequential(\n",
    "            nn.Linear(None, None),\n",
    "            nn.ReLU())\n",
    "        self.fc_block2 = nn.Sequential(\n",
    "            nn.Linear(None,None),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        print('Output 1: ', x.shape)\n",
    "        x = self.conv_block2(x)\n",
    "        print('Output 2: ', x.shape)\n",
    "        x = x.view(-1, None)\n",
    "        x = self.fc_block1(x)\n",
    "        print('Output 3: ', x.shape)\n",
    "        x = self.fc_block2(x)\n",
    "        print('Output 4: ', x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# input image\n",
    "input_image = torch.randint(0, 255, (1, 1, 64, 64)).float()\n",
    "print('Input: ', input_image.shape)\n",
    "# create CNN model\n",
    "net = CNNModel2()\n",
    "# predict output for input_image\n",
    "o = net(input_image)\n",
    "# shape is as expected\n",
    "print('Final output: ', o.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
